{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -o data/fra-eng.zip http://www.manythings.org/anki/fra-eng.zip\n",
    "#!unzip data/fra-eng.zip -d data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'fra.txt' does not exist: b'fra.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b8fd0f7482d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fra.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'fra.txt' does not exist: b'fra.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/fra.txt\",sep='\\t',header= None,encoding=\"utf-8\")\n",
    "ang = list(data[0])\n",
    "fra = list(data[1])\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.fr import French\n",
    "\n",
    "nlp = English()\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "nlp_f = French()\n",
    "tokenizer_f = nlp_f.Defaults.create_tokenizer(nlp_f)\n",
    "\n",
    "ang = [['<S>'] + [token.string.strip() for token in tokenizer(text.lower())] + ['</S>'] for text in ang][:10000]\n",
    "\n",
    "fra = [['<S>'] + [token.string.strip() for token in tokenizer_f(text.lower())] + ['</S>'] for text in fra][:10000]\n",
    "\n",
    "print(ang[1000])\n",
    "print(fra[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "EMBEDDING_SIZE = 120\n",
    "w2v = Word2Vec(ang, size=EMBEDDING_SIZE, window=10, min_count=1, negative=10, workers=10)\n",
    "word_map = {}\n",
    "word_map[\"<PAD>\"] = 0\n",
    "word_vectors = [np.zeros((EMBEDDING_SIZE,))]\n",
    "for i, w in enumerate([w for w in w2v.wv.vocab]):\n",
    "    word_map[w] = i+1\n",
    "    word_vectors.append(w2v.wv[w])\n",
    "word_vectors = np.vstack(word_vectors)\n",
    "\n",
    "w2v = Word2Vec(fra, size=EMBEDDING_SIZE, window=10, min_count=1, negative=10, workers=10)\n",
    "word_map_fr = {}\n",
    "word_map_fr[\"<PAD>\"] = 0\n",
    "word_vectors_fr = [np.zeros((EMBEDDING_SIZE,))]\n",
    "for i, w in enumerate([w for w in w2v.wv.vocab]):\n",
    "    word_map_fr[w] = i+1\n",
    "    word_vectors_fr.append(w2v.wv[w])\n",
    "word_vectors_fr = np.vstack(word_vectors_fr)\n",
    "i2w = dict(zip([*word_map_fr.values()],[*word_map_fr]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding, word2id and shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(a,shift = False):\n",
    "    shape = len(a)\n",
    "    max_s = max([len(x) for x in a])\n",
    "    token = np.zeros((shape,max_s+1),dtype = np.int)\n",
    "    mask  =  np.zeros((shape,max_s+1),dtype = np.int)\n",
    "    for i,o in enumerate(a):\n",
    "        token[i,:len(o)] = o\n",
    "        mask[i,:len(o)] = 1\n",
    "    if shift:\n",
    "        return token[:,:-1],token[:,1:],mask[:,1:],max_s\n",
    "    else:\n",
    "        return token[:,:-1],mask[:,:-1],max_s\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ang_tok,ang_mask,ang_pl = pad([[word_map[w] for w in text] for text in ang])\n",
    "fra_tok,fra_tok_t,fra_mask,fr_pl = pad([[word_map_fr[w] for w in text] for text in fra],shift = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers,Model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "import tensorflow as tf\n",
    "\n",
    "class S2S(tf.keras.Model):\n",
    "    def __init__(self,Win,Wout,i2w):\n",
    "      \n",
    "        super(S2S, self).__init__() \n",
    "        \n",
    "        self.nv_in = Win.shape[0]\n",
    "        self.r = Win.shape[1]\n",
    "        self.nv_out = Wout.shape[0]\n",
    "        \n",
    "        self.i2w = i2w\n",
    "        \n",
    "        self.Win = layers.Embedding(self.nv_in,self.r)\n",
    "        self.Win.build((None, ))\n",
    "        self.Win.set_weights([Win])\n",
    "        self.Win.trainable = True\n",
    "\n",
    "        self.Wout = layers.Embedding(self.nv_out,self.r)\n",
    "        self.Wout.build((None, ))\n",
    "        self.Wout.set_weights([Wout])\n",
    "        self.Wout.trainable = True\n",
    "        \n",
    "        self.encoder = layers.GRU(self.r,dropout=0.2)\n",
    "        \n",
    "        self.decoder = layers.GRU(self.r, return_sequences=True, return_state=True,dropout=0.2)\n",
    "        \n",
    "        self.mapper = layers.Dense(self.nv_out,activation = \"softmax\")\n",
    "\n",
    "    @tf.function\n",
    "    def call(self,x,x_mask,x_out):\n",
    "        x = self.Win(x)\n",
    "        x_mask = tf.cast(x_mask,dtype=bool)\n",
    "    \n",
    "        x = self.encoder(x,mask=x_mask)   \n",
    "        \n",
    "        x_out = self.Wout(x_out)\n",
    "        out,_ = self.decoder(x_out, initial_state=x)\n",
    "        \n",
    "        probs = self.mapper(out)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def generate(self,start_emb,stop_emb,x,x_mask):\n",
    "        aout = []\n",
    "        \n",
    "        x = tf.expand_dims(x,axis=0)\n",
    "        x_mask = tf.expand_dims(x_mask,axis=0)\n",
    "\n",
    "        x_mask = tf.cast(x_mask,dtype=bool)\n",
    "        \n",
    "        x = self.Win(x)\n",
    "\n",
    "        x = self.encoder(x,mask=x_mask) \n",
    "        \n",
    "        x_out = tf.expand_dims(tf.expand_dims(self.Wout(start_emb),axis = 0),axis = 0)         \n",
    "        _,out = self.decoder(x_out, initial_state=x) \n",
    "        probs = tf.squeeze(self.mapper(out))\n",
    "        \n",
    "        x_out = tf.math.argmax(probs)\n",
    "        val,argval = tf.nn.top_k(probs, k=2, sorted=True, name=None)\n",
    "        x_out = argval.numpy()[0]\n",
    "        aout.append(self.i2w[x_out])\n",
    "        \n",
    "        for i in range(10):\n",
    "            x_out = tf.expand_dims(tf.expand_dims(self.Wout(tf.constant(x_out)),axis = 0),axis = 0)     \n",
    "            _,out = self.decoder(x_out, initial_state=out)  \n",
    "            \n",
    "            probs = tf.squeeze(self.mapper(out))\n",
    "            val,argval = tf.nn.top_k(probs, k=2, sorted=True, name=None)\n",
    "            x_out = argval.numpy()[0]\n",
    "            aout.append(self.i2w[x_out])\n",
    "            \n",
    "            if x_out == stop_emb:\n",
    "                break\n",
    "            \n",
    "        return aout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(model,loss_f,x,x_mask,x_out,y_out,y_out_mask):\n",
    "    \n",
    "    \n",
    "    probs = model(x,x_mask,x_out)\n",
    "    \n",
    "    y_true= tf.boolean_mask(y_out,y_out_mask)\n",
    "    y_pred = tf.boolean_mask(probs,y_out_mask) \n",
    "    \n",
    "    return loss_f(y_true,y_pred),y_true,y_pred\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def compute_apply_gradients(model,loss_f,x,x_mask,x_out,y_out,y_out_mask, optimizer):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        loss,label,prediction= compute_loss(model, loss_f,x,x_mask,x_out,y_out,y_out_mask)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss,label,prediction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "\n",
    "X = np.hstack([ang_tok,ang_mask,fra_tok])\n",
    "print(X.shape)\n",
    "Y = np.hstack([fra_tok_t,fra_mask])\n",
    "print(Y.shape)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.80, random_state=101)\n",
    "\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train,Y_train)).batch(batch_size)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test,Y_test)).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model declaration and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = S2S(word_vectors,word_vectors_fr,i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "loss_f = tf.keras.losses.CategoricalCrossentropy()\n",
    "    \n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
    "loss_f = tf.keras.losses.CategoricalCrossentropy()\n",
    "epochs = 30\n",
    "\n",
    "checkpoint_dir = 'training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 model=model)\n",
    "\n",
    "tr_loss = []\n",
    "te_loss = []\n",
    "tr_acc = []\n",
    "te_acc = []\n",
    "    \n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(epoch,flush=True,)\n",
    "\n",
    "    for x,y in tqdm(train_data):\n",
    "        \n",
    "        x,x_mask,x_out = tf.split(x,[ang_pl,ang_pl,fr_pl],axis=1)\n",
    "        y_out,y_out_mask = tf.split(y,2,axis=1)\n",
    "        \n",
    "        y_out_onehot = tf.one_hot(y_out,depth = word_vectors_fr.shape[0])\n",
    "        \n",
    "        loss,label,prediction = compute_apply_gradients(model,loss_f,x,x_mask,x_out,y_out_onehot,y_out_mask,optimizer)\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(label, prediction)\n",
    "        \n",
    "    for x,y in tqdm(test_data):\n",
    "\n",
    "        x,x_mask,x_out = tf.split(x,[ang_pl,ang_pl,fr_pl],axis=1)\n",
    "        y_out,y_out_mask = tf.split(y,2,axis=1)\n",
    "\n",
    "        y_out_onehot = tf.one_hot(y_out,depth = word_vectors_fr.shape[0])\n",
    "\n",
    "        loss,label,prediction = compute_loss(model,loss_f,x,x_mask,x_out,y_out_onehot,y_out_mask)\n",
    "        test_loss(loss)\n",
    "        test_accuracy(label, prediction)\n",
    "                \n",
    "    print(\" \".join(ang[1000]))\n",
    "    print(\" \".join(model.generate(word_map_fr[\"<S>\"],word_map_fr[\"</S>\"],ang_tok[1000,:],ang_mask[1000,:])))\n",
    "    \n",
    "    print(\" \".join(ang[5000]))\n",
    "    print(\" \".join(model.generate(word_map_fr[\"<S>\"],word_map_fr[\"</S>\"],ang_tok[5000,:],ang_mask[5000,:])))\n",
    "    \n",
    "    print(\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Loss: {test_loss.result()}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}')\n",
    "    \n",
    "    tr_loss.append(train_loss.result())\n",
    "    te_loss.append(test_loss.result())\n",
    "    tr_acc.append(train_accuracy.result())\n",
    "    te_acc.append(test_accuracy.result())\n",
    "    if epoch % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(tr_loss, c=\"orange\")\n",
    "plt.plot(te_loss, c=\"blue\")\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(tr_acc, c=\"orange\")\n",
    "plt.plot(te_acc, c=\"red\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
