{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-272964f75d59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/fra.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfra\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdependency\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhard_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mmissing_dependencies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;31m# do this after everything else, to minimize the chance of this misleadingly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;31m# appearing in an import-time traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_add_newdocs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_dtype_ctypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\_add_newdocs.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   4613\u001b[0m     \u001b[0mformat_float_scientific\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4614\u001b[0m     \u001b[0mformat_float_positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4615\u001b[1;33m     \"\"\")\n\u001b[0m\u001b[0;32m   4616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoi\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\function_base.py\u001b[0m in \u001b[0;36madd_newdoc\u001b[1;34m(place, obj, doc)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \"\"\"\n\u001b[0;32m    452\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m         \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[0madd_docstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoi\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoi\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoi\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoi\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoi\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoi\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\antoi\\appdata\\local\\programs\\python\\python37\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/fra.txt\",sep='\\t',header= None)\n",
    "ang = list(data[0])\n",
    "fra = list(data[1])\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.fr import French\n",
    "\n",
    "nlp = English()\n",
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
    "nlp_f = French()\n",
    "tokenizer_f = nlp_f.Defaults.create_tokenizer(nlp_f)\n",
    "\n",
    "ang = [['<S>'] + [token.string.strip() for token in tokenizer(text.lower())] + ['</S>'] for text in ang][:10000]\n",
    "\n",
    "fra = [['<S>'] + [token.string.strip() for token in tokenizer_f(text.lower())] + ['</S>'] for text in fra][:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding, word2id and shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "EMBEDDING_SIZE = 120\n",
    "w2v = Word2Vec(ang, size=EMBEDDING_SIZE, window=10, min_count=1, negative=10, workers=10)\n",
    "word_map = {}\n",
    "word_map[\"<PAD>\"] = 0\n",
    "word_vectors = [np.zeros((EMBEDDING_SIZE,))]\n",
    "for i, w in enumerate([w for w in w2v.wv.vocab]):\n",
    "    word_map[w] = i+1\n",
    "    word_vectors.append(w2v.wv[w])\n",
    "word_vectors = np.vstack(word_vectors)\n",
    "\n",
    "w2v = Word2Vec(fra, size=EMBEDDING_SIZE, window=10, min_count=1, negative=10, workers=10)\n",
    "word_map_fr = {}\n",
    "word_map_fr[\"<PAD>\"] = 0\n",
    "word_vectors_fr = [np.zeros((EMBEDDING_SIZE,))]\n",
    "for i, w in enumerate([w for w in w2v.wv.vocab]):\n",
    "    word_map_fr[w] = i+1\n",
    "    word_vectors_fr.append(w2v.wv[w])\n",
    "word_vectors_fr = np.vstack(word_vectors_fr)\n",
    "i2w = dict(zip([*word_map_fr.values()],[*word_map_fr]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(a,shift = False):\n",
    "    shape = len(a)\n",
    "    max_s = max([len(x) for x in a])\n",
    "    if shift:\n",
    "        token = np.zeros((shape,max_s+1),dtype = np.int)\n",
    "        mask  =  np.zeros((shape,max_s+1),dtype = np.int)\n",
    "        for i,o in enumerate(a):\n",
    "            token[i,:len(o)] = o\n",
    "            mask[i,:len(o)] = 1\n",
    "        return token[:,1:],mask[:,1:],max_s       \n",
    "    else:            \n",
    "        token = np.zeros((shape,max_s),dtype = np.int)\n",
    "        mask  =  np.zeros((shape,max_s),dtype = np.int)\n",
    "        for i,o in enumerate(a):\n",
    "            token[i,:len(o)] = o\n",
    "            mask[i,:len(o)] = 1\n",
    "        return token,mask,max_s \n",
    "    \n",
    "ang_tok,ang_mask,ang_pl = pad([[word_map[w] for w in text] for text in ang])\n",
    "fra_tok,fra_mask,fr_pl = pad([[word_map_fr[w] for w in text] for text in fra])\n",
    "fra_toks_s,fra_mask_s,_ = pad([[word_map_fr[w] for w in text] for text in fra],shift = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers,Model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "import tensorflow as tf\n",
    "\n",
    "class S2S(tf.keras.Model):\n",
    "    def __init__(self,Win,Wout,i2w):\n",
    "      \n",
    "        super(S2S, self).__init__() \n",
    "        \n",
    "        self.nv_in = Win.shape[0]\n",
    "        self.r = Win.shape[1]\n",
    "        self.nv_out = Wout.shape[0]\n",
    "        \n",
    "        self.i2w = i2w\n",
    "        \n",
    "        self.Win = layers.Embedding(self.nv_in,self.r)\n",
    "        self.Win.build((None, ))\n",
    "        self.Win.set_weights([Win])\n",
    "        self.Win.trainable = True\n",
    "\n",
    "        self.Wout = layers.Embedding(self.nv_out,self.r)\n",
    "        self.Wout.build((None, ))\n",
    "        self.Wout.set_weights([Wout])\n",
    "        self.Wout.trainable = True\n",
    "        \n",
    "        self.encoder = layers.GRU(self.r, return_sequences=True, return_state=True,dropout=0.2)\n",
    "        \n",
    "        self.decoder = layers.GRU(self.r, return_sequences=True, return_state=True,dropout=0.2)\n",
    "        \n",
    "        self.mapper = layers.Dense(self.nv_out,activation = \"softmax\")\n",
    "\n",
    "        self.attention = layers.Attention()\n",
    "\n",
    "        self.H = layers.Dense(self.r*2,activation = \"tanh\")\n",
    "\n",
    "        self.W = layers.Dense(self.r,activation = \"relu\")\n",
    "\n",
    "    @tf.function\n",
    "    def encode(self,x,x_mask):\n",
    "        \n",
    "        x = self.Win(x)\n",
    "        x_mask = tf.cast(x_mask,dtype=bool)\n",
    "    \n",
    "        hidden_seq,hidden_last = self.encoder(x,mask=x_mask)\n",
    "\n",
    "        return hidden_seq,hidden_last\n",
    "    \n",
    "    @tf.function\n",
    "    def decode(self,encoder_seq,encoder_mask,decoder_last,context_last,x_out,attention = False):\n",
    "\n",
    "        x_out = self.Wout(x_out)\n",
    "\n",
    "        input_decoder = tf.concat([x_out,context_last],2)\n",
    "\n",
    "        encoder_mask = tf.cast(encoder_mask,dtype=bool)\n",
    "\n",
    "        _,decoder_last = self.decoder(input_decoder, initial_state=decoder_last)\n",
    "\n",
    "        decoder_last = tf.expand_dims(decoder_last,1)\n",
    "        \n",
    "        query = self.W(decoder_last)\n",
    "\n",
    "        key = encoder_seq \n",
    "        value = encoder_seq \n",
    "            \n",
    "        context_vector = self.attention([query,key,value],mask = [None,encoder_mask])\n",
    "\n",
    "        probs = self.mapper(self.H(tf.concat([decoder_last,context_vector],2)))\n",
    "\n",
    "        decoder_last = tf.squeeze(decoder_last)\n",
    "        \n",
    "        if attention == True:\n",
    "            decoder_last = tf.expand_dims(decoder_last,axis = 0)\n",
    "            scores = tf.matmul(query, key, transpose_b=True)\n",
    "            weights = tf.nn.softmax(scores)\n",
    "            return weights,decoder_last,context_vector\n",
    "        else:\n",
    "            return probs,decoder_last,context_vector\n",
    "\n",
    " \n",
    "    def att_wei(self,x,x_mask,x_out,x_out_mask):\n",
    "        \n",
    "        x = tf.expand_dims(x,axis=0)\n",
    "        x_mask = tf.expand_dims(x_mask,axis=0)\n",
    "        x_out = tf.expand_dims(x_out,axis=0)\n",
    "        x_out_mask = tf.expand_dims(x_out_mask,axis=0)\n",
    "        \n",
    "        mask = tf.cast(tf.matmul(tf.transpose(x_out_mask),x_mask),dtype=tf.bool)\n",
    "\n",
    "        encoder_seq,hidden_last = model.encode(x,x_mask)\n",
    "        context_last = tf.zeros([x.shape[0],model.r])\n",
    "        context_last = tf.expand_dims(context_last,1)\n",
    "\n",
    "        input_0 = tf.gather(x_out, [0], axis=1)\n",
    "        \n",
    "        \n",
    "        weights,hidden_last,context_last = model.decode(encoder_seq,x_mask,hidden_last,context_last,input_0,attention = True)\n",
    "        pro = []\n",
    "        \n",
    "        pro.append(weights)\n",
    "\n",
    "        for t in range(1,x_out.shape[1]):\n",
    "            input_0 = tf.gather(x_out, [t], axis=1)\n",
    "            weights,hidden_last,context_last = model.decode(encoder_seq,x_mask,hidden_last,context_last,input_0,attention = True)\n",
    "            pro.append(weights)\n",
    "\n",
    "        pro = tf.concat(pro,1)\n",
    "        \n",
    "        out = tf.boolean_mask(tf.squeeze(pro),mask)\n",
    "        \n",
    "        out = tf.reshape(out,(tf.reduce_sum(x_out_mask),tf.reduce_sum(x_mask)))\n",
    "                \n",
    "        return out.numpy() \n",
    "\n",
    "\n",
    "    \n",
    "    def generate(self,start_emb,x,x_mask):\n",
    "        aout = []\n",
    "        \n",
    "        x = tf.expand_dims(x,axis=0)\n",
    "        x_mask = tf.expand_dims(x_mask,axis=0)\n",
    "\n",
    "        encoder_seq,hidden_last = model.encode(x,x_mask)\n",
    "        context_last = tf.zeros([x.shape[0],model.r])\n",
    "        context_last = tf.expand_dims(context_last,1)\n",
    "       \n",
    "        input_0 = tf.expand_dims(tf.expand_dims(start_emb,axis=0),axis=0)\n",
    "\n",
    "        probs,hidden_last,context_last = model.decode(encoder_seq,x_mask,hidden_last,context_last,input_0)\n",
    "\n",
    "        val,argval = tf.nn.top_k(tf.squeeze(probs), k=2, sorted=True, name=None)\n",
    "        x_out = argval.numpy()[0]\n",
    "        aout.append(self.i2w[x_out])\n",
    "    \n",
    "        for t in range(10):\n",
    "            hidden_last = tf.expand_dims(hidden_last,axis=0)\n",
    "            input_0 = tf.expand_dims(tf.expand_dims(x_out,axis=0),axis=0)\n",
    "            probs,hidden_last,context_last = model.decode(encoder_seq,x_mask,hidden_last,context_last,input_0)\n",
    "            \n",
    "            val,argval = tf.nn.top_k(tf.squeeze(probs), k=2, sorted=True, name=None)\n",
    "            x_out = argval.numpy()[0]\n",
    "            aout.append(self.i2w[x_out])\n",
    "                \n",
    "        return aout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "\n",
    "def plot_weight(vegetables,farmers,harvest):\n",
    "    \n",
    "    vegetables = vegetables[1:]\n",
    "    harvest = harvest[0:-1,:]\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(harvest)\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(farmers)))\n",
    "    ax.set_yticks(np.arange(len(vegetables)))\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(farmers)\n",
    "    ax.set_yticklabels(vegetables)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(len(vegetables)):\n",
    "        for j in range(len(farmers)):\n",
    "            text = ax.text(j, i, harvest[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax.set_title(\"Attention\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(model,loss_f,x,x_mask,x_out,y_onehot,y_mask):\n",
    "    pro = []\n",
    "    \n",
    "    encoder_seq,hidden_last = model.encode(x,x_mask)\n",
    "    context_last = tf.zeros([x.shape[0],model.r])\n",
    "    context_last = tf.expand_dims(context_last,1)\n",
    "    \n",
    "    input_0 = tf.gather(x_out, [0], axis=1)\n",
    "    probs,hidden_last,context_last = model.decode(encoder_seq,x_mask,hidden_last,context_last,input_0)\n",
    "\n",
    "    pro.append(probs)\n",
    "\n",
    "    for t in range(1,y_onehot.shape[1]):\n",
    "        input_0 = tf.gather(x_out, [t], axis=1)\n",
    "        probs,hidden_last,context_last = model.decode(encoder_seq,x_mask,hidden_last,context_last,input_0)\n",
    "        pro.append(probs)\n",
    "        \n",
    "    pro = tf.concat(pro,1)\n",
    "\n",
    "    y_true= tf.boolean_mask(y_onehot,y_mask)\n",
    "    y_pred = tf.boolean_mask(pro,y_mask)\n",
    "    \n",
    "    \n",
    "    return loss_f(y_true,y_pred),y_true,y_pred\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def compute_apply_gradients(model,loss_f,x,x_mask,x_out,y_onehot,y_mask,optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        loss,label,prediction = compute_loss(model,loss_f,x,x_mask,x_out,y_onehot,y_mask)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss,label,prediction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "X = np.hstack([ang_tok,ang_mask])\n",
    "print(X.shape)\n",
    "Y = np.hstack([fra_tok,fra_toks_s,fra_mask_s])\n",
    "print(Y.shape)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.80, random_state=101)\n",
    "\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train,Y_train)).batch(batch_size)\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test,Y_test)).batch(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model declaration and Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "model = S2S(word_vectors,word_vectors_fr,i2w)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "checkpoint_dir = 'training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 model=model)\n",
    "\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "loss_f = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "tr_loss = []\n",
    "te_loss = []\n",
    "tr_acc = []\n",
    "te_acc = []\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(epoch,flush=True,)\n",
    "\n",
    "    for x,y in tqdm(train_data):\n",
    "        \n",
    "        x,x_mask = tf.split(x,2,axis=1)\n",
    "        x_out,y,y_mask = tf.split(y,3,axis=1)\n",
    "        \n",
    "        y_onehot = tf.one_hot(y,depth = word_vectors_fr.shape[0])\n",
    "        loss,label,prediction = compute_apply_gradients(model,loss_f,x,x_mask,x_out,y_onehot,y_mask,optimizer)\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(label, prediction)\n",
    "        \n",
    "    for x,y in tqdm(test_data):\n",
    "\n",
    "        x,x_mask = tf.split(x,2,axis=1)\n",
    "        x_out,y,y_mask = tf.split(y,3,axis=1)\n",
    "        \n",
    "        y_out_onehot = tf.one_hot(y,depth = word_vectors_fr.shape[0])\n",
    "        loss,label,prediction = compute_loss(model,loss_f,x,x_mask,x_out,y_out_onehot,y_mask)\n",
    "\n",
    "        test_loss(loss)\n",
    "        test_accuracy(label, prediction)\n",
    "      \n",
    "    print(\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Loss: {test_loss.result()}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}')\n",
    "    \n",
    "    tr_loss.append(train_loss.result())\n",
    "    te_loss.append(test_loss.result())\n",
    "    tr_acc.append(train_accuracy.result())\n",
    "    te_acc.append(test_accuracy.result())\n",
    "    if epoch % 2 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(\" \".join(ang[1000]))\n",
    "        print(\" \".join(model.generate(word_map_fr[\"<S>\"],ang_tok[1000,:],ang_mask[1000,:])))\n",
    "\n",
    "        print(\" \".join(ang[5000]))\n",
    "        print(\" \".join(model.generate(word_map_fr[\"<S>\"],ang_tok[5000,:],ang_mask[5000,:])))\n",
    "\n",
    "        wei = model.att_wei(ang_tok[5000,:],ang_mask[5000,:],fra_tok[5000,:],fra_mask[5000,:])\n",
    "        plot_weight(fra[5000],ang[5000],wei)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(tr_loss, c=\"orange\")\n",
    "plt.plot(te_loss, c=\"blue\")\n",
    "plt.title(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(tr_acc, c=\"orange\")\n",
    "plt.plot(te_acc, c=\"red\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
